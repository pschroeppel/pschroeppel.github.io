<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Philipp Schröppel</title>

    <meta name="author" content="Philipp Schröppel">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Philipp Schröppel
                </p>
                <p>
                  I am a PhD student in the <a href="https://lmb.informatik.uni-freiburg.de/index.php">Computer Vision Group</a> at the University of Freiburg, headed by <a href="https://lmb.informatik.uni-freiburg.de/people/brox/index.html">Prof. Thomas Brox</a>.
                </p>
                <p>
                  My broad research interest is 3D reconstruction in terms of 3d geometry, ego‑motion and object motion. A particular focus is robust application on arbitrary real‑world data. 
                </p>
                <p>
                  Recently, I worked on 3D generation using diffusion models. Currently, I am interested in learning more about diffusion models, and in
                  using scene priors learnt by diffusion models for 3D reconstruction.
                </p>
                <p style="text-align:center">
                  <a href="mailto:schroepp@cs.uni-freiburg.de">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Schroeppel_Philipp_EN.pdf">CV</a> &nbsp;/&nbsp;
                  <!--<a href="data/bio.txt">Bio</a> &nbsp;/&nbsp;-->
                  <a href="https://scholar.google.com/citations?user=mHyYWLsAAAAJ&hl=de">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/PhilSchroeppel">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/pschroeppel/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/portrait.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/portrait.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/npcd.gif" alt="NPCD" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://neural-point-cloud-diffusion.github.io/">
                  <span class="papertitle">Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation</span>
                </a>
                <br>
                <strong>Philipp Schröppel</strong>,
                <a href="https://geometric-rl.mpi-inf.mpg.de/people/Wewer.html">Christopher Wewer</a>,
                <a href="https://janericlenssen.github.io/">Jan Eric Lenssen</a>,
                <a href="https://cvmp.cs.uni-saarland.de/people/#eddy-ilg">Eddy Ilg</a>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/brox/">Thomas Brox</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2312.14124.pdf">Paper</a> / <a href="https://neural-point-cloud-diffusion.github.io">Project page</a> / <a href="https://github.com/lmb-freiburg/neural-point-cloud-diffusion">Code</a> (coming soon!) / <a href="data/SchroeppelCVPR2024.bib">Bibtex</a>
                <p>We generate 3D shape and appearance of objects via diffusion on neural point clouds (points with a 3D position and a learned feature). As the point positions represent the coarse shape and the features represent the appearance, shape and appearance can be generated separately.</p>
              </td>
            </tr>
    
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/rmvd.png" alt="RMVD" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/lmb-freiburg/robustmvd">
                  <span class="papertitle">A Benchmark and a Baseline for Robust Multi-view Depth Estimation</span>
                </a>
                <br>
                <strong>Philipp Schröppel</strong>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/bechtolj/">Jan Bechtold</a>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/amiranas/">Artemij Amiranashvili</a>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/brox/">Thomas Brox</a>
                <br>
                <em>3DV</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2209.06681.pdf">Paper</a> / <a href="https://github.com/lmb-freiburg/robustmvd">Code</a> / <a href="data/Schroeppel3DV2022.bib">Bibtex</a>
                <p>We introduce a benchmark for robust zero-shot multi-view depth estimation and evaluate many recent models. We present a new model that works more robustly across data from different domains and can serve as a baseline for future evaluations on the benchmark.</p>
              </td>
            </tr>
    
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/kitti.gif" alt="SF2SE3" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2209.08532">
                  <span class="papertitle">SF2SE3: Clustering Scene Flow into SE (3)-Motions via Proposal and Selection</span>
                </a>
                <br>
                <a href="https://lmb.informatik.uni-freiburg.de/people/sommerl/">Leonhard Sommer</a>,
                <strong>Philipp Schröppel</strong>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/brox/">Thomas Brox</a>
                <br>
                <em>GCPR</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2209.08532.pdf">Paper</a> / <a href="https://github.com/lmb-freiburg/sf2se3">Code</a> / <a href="data/SommerGCPR2022.bib">Bibtex</a>
                <p>We propose SF2SE3, an approach estimate a moving object segmentation and corresponding SE(3) motions from optical flow and depth. It works by iteratively sampling motion proposals and selecting the best ones with respect to a maximum coverage formulation.</p>
              </td>
            </tr>
    
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/semi_sup_disp_deep_feat_recon.png" alt="SF2SE3" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2106.00318">
                  <span class="papertitle">Semi-Supervised Disparity Estimation with Deep Feature Reconstruction</span>
                </a>
                <br>
                <a href="http://webdiis.unizar.es/~juliagv/">Julia Guerrero-Viu</a>,
                <a href="https://serizba.github.io/">Sergio Izquierdo</a>,
                <strong>Philipp Schröppel</strong>,
                <a href="https://lmb.informatik.uni-freiburg.de/people/brox/">Thomas Brox</a>
                <br>
                <em>CVPR Women in Computer Vision Workshop</em>, 2021
                <br>
                <a href="https://arxiv.org/pdf/2106.00318.pdf">Paper</a>
                <p>We train a network for disparity estimation in a semi-supervised fashion on labeled synthetic data and unlabeled real data. For supervision on the unlabeled data, we explore a deep feature reconstruction loss, instead of the commonly used photometric loss.</p>
              </td>
            </tr>

          </tbody></table>            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  The template for this website comes from <a href="http://jonbarron.info">Jon Barron</a>. Thanks!
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
